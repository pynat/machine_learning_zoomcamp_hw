{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37630851-45cf-4148-be92-2a7cae0f11ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3047b8-a716-41b3-b03d-28beb6ea121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/var/folders/_4/617xrc517f5dk8c2t0gbnksr0000gn/T/tmphxrvyec1'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 200, 200, 3), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  5203616768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5203621696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5203866752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5203868688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5204041168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5204038000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1733757540.656042 2405574 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1733757540.663816 2405574 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2024-12-09 16:19:00.670060: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/_4/617xrc517f5dk8c2t0gbnksr0000gn/T/tmphxrvyec1\n",
      "2024-12-09 16:19:00.670344: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2024-12-09 16:19:00.670348: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /var/folders/_4/617xrc517f5dk8c2t0gbnksr0000gn/T/tmphxrvyec1\n",
      "I0000 00:00:1733757540.682006 2405574 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "2024-12-09 16:19:00.683804: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2024-12-09 16:19:01.006610: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /var/folders/_4/617xrc517f5dk8c2t0gbnksr0000gn/T/tmphxrvyec1\n",
      "2024-12-09 16:19:01.011450: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 341390 microseconds.\n",
      "2024-12-09 16:19:01.036655: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the TF-Lite model: 76.58 MB\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel('ERROR') \n",
    "\n",
    "# load keras model\n",
    "model = tf.keras.models.load_model(\"model_2024_hairstyle.keras\")\n",
    "\n",
    "# convert keras model to tf-lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# save tf-lite model\n",
    "with open(\"model_2024_hairstyle.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "# print size of converted model\n",
    "model_size = len(tflite_model) / (1024 * 1024) \n",
    "print(f\"Size of the TF-Lite model: {model_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b64e990-e84f-41da-9492-10d9f1079120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da086d7-d3c5-42dd-9d49-f4171e0a48aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Index: 0\n",
      "Output Index: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# load tf model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model_2024_hairstyle.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(f\"Input Index: {input_details[0]['index']}\")\n",
    "print(f\"Output Index: {output_details[0]['index']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c4f589-011d-4e4c-a8ea-a3e2c58e5480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6613302-05ce-41b1-8604-6c2193d60793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image prepared successfully!\n",
      "The target size should be: (200, 200)\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from urllib import request\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# functions to download and resize image\n",
    "def download_image(url):\n",
    "    with request.urlopen(url) as resp:\n",
    "        buffer = resp.read()\n",
    "    stream = BytesIO(buffer)\n",
    "    img = Image.open(stream)\n",
    "    return img\n",
    "\n",
    "def prepare_image(img, target_size):\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize(target_size, Image.NEAREST)\n",
    "    return img\n",
    "\n",
    "# download image\n",
    "url = \"https://habrastorage.org/webt/yf/_d/ok/yf_dokzqy3vcritme8ggnzqlvwa.jpeg\"\n",
    "image = download_image(url)\n",
    "\n",
    "# prepare image with target size from prev homework\n",
    "target_size = (200, 200)  \n",
    "image = prepare_image(image, target_size)\n",
    "print(\"Image prepared successfully!\")\n",
    "print(f\"The target size should be: {target_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4177cb27-ed43-4ab3-bdcc-64517f1d21ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First pixel value, R channel: 0.24\n"
     ]
    }
   ],
   "source": [
    "# convert image to numpy array\n",
    "image_array = np.array(image) \n",
    "\n",
    "# normalize array (scale values to [0, 1])\n",
    "image_array = image_array / 255.0\n",
    "image_array = image_array.astype(np.float32)\n",
    "\n",
    "# expand dimensions to add batch size dimension\n",
    "input_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "print(f\"First pixel value, R channel: {input_array[0, 0, 0, 0]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a933391-16d7-4436-a3f2-7f8a0177f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ffa3d7-712b-4e09-9aa5-83d78d851458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: [[0.8937725]]\n"
     ]
    }
   ],
   "source": [
    "# Add batch dimension\n",
    "input_data = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "# Run the inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensor\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(f\"Model output: {output_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca3a308d-87aa-43da-95bf-e130a6ad5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b00046-e913-42a1-9463-bdf93c24d5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac6cb7-2c48-40bd-8d97-fb7630131e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
